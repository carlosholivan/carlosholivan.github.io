<!DOCTYPE HTML>
<html>
	<head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Carlos Hern&aacute;ndez Oliv&aacute;n</title>
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="description" content="" />
	<meta name="keywords" content="" />
	<meta name="author" content="" />

  <!-- Facebook and Twitter integration -->
	<meta property="og:title" content=""/>
	<meta property="og:image" content=""/>
	<meta property="og:url" content=""/>
	<meta property="og:site_name" content=""/>
	<meta property="og:description" content=""/>
	<meta name="twitter:title" content="" />
	<meta name="twitter:image" content="" />
	<meta name="twitter:url" content="" />
	<meta name="twitter:card" content="" />

	<!-- Place favicon.ico and apple-touch-icon.png in the root directory -->
	<link rel="shortcut icon" href="favicon.ico">

	<link href="https://fonts.googleapis.com/css?family=Karla:400,700" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Playfair+Display:400,400i,700" rel="stylesheet">
	
	<!-- Animate.css -->
	<link rel="stylesheet" href="../css/animate.css">
	<!-- Icomoon Icon Fonts-->
	<link rel="stylesheet" href="../css/icomoon.css">
	<!-- Bootstrap  -->
	<link rel="stylesheet" href="../css/bootstrap.css">
	<!-- Owl Carousel -->
	<link rel="stylesheet" href="../css/owl.carousel.min.css">
	<link rel="stylesheet" href="../css/owl.theme.default.min.css">
	<!-- Magnific Popup -->
	<link rel="stylesheet" href="../css/magnific-popup.css">

	<link rel="stylesheet" href="../css/style.css">


	<!-- Modernizr JS -->
	<script src="js/modernizr-2.6.2.min.js"></script>
	<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
	<!-- FOR IE9 below -->
	<!--[if lt IE 9]>
	<script src="js/respond.min.js"></script>
	<![endif]-->

    

	</head>
	<body>

	<!-- Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=G-3CDKKZRCVT"></script>
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());
		gtag('config', 'G-3CDKKZRCVT');
	</script>

	<nav id="colorlib-main-nav" role="navigation">
		<a href="#" class="js-colorlib-nav-toggle colorlib-nav-toggle active"><i></i></a>
		<div class="js-fullheight colorlib-table">
			<div class="colorlib-table-cell js-fullheight">
				<div class="row">
					<div class="col-md-12">
						<div class="form-group">
							<input type="text" class="form-control" id="search" placeholder="Enter any key to search...">
							<button type="submit" class="btn btn-primary"><i class="icon-search3"></i></button>
						</div>
					</div>
				</div>
				<div class="row">
					<div class="col-md-12">
						<ul>
							<li><a href="../index.html">Home</a></li>
							<li><a href="../curriculum.html">CV</a></li>
							<li><a href="../publications.html">Publications</a></li>
							<li><a href="../talks.html">Talks</a></li>
							<li class="active"><a href="../teaching.html">Teaching</a></li>
							<li><a href="../community-service.html">Community Service</a></li>
							<li><a href="../blog.html">Blog</a></li>
							<li><a href="../others.html">Others</a></li>
							<li><a href="../about.html">About</a></li>
							<li><a href="../contact.html">Contact</a></li>
						</ul>
					</div>
				</div>
			</div>
		</div>
	</nav>
	
	<div id="colorlib-page">
		<header>
			<div class="container">
				<div class="row">
					<div class="col-md-12">
						<div class="colorlib-navbar-brand">
							<a class="colorlib-logo" href="../index.html"><span>Ch</span><span>o</span></a>
						</div>
						<a href="#" class="js-colorlib-nav-toggle colorlib-nav-toggle"><i></i></a>
					</div>
				</div>
			</div>
		</header>

        <div id="colorlib-blog">
            <div class="container">
                <div class="row text-center">
                    <h2 class="bold">Paper Title</h2>
                </div>
                <div class="row">
                    <div class="col-md-12">
                        <div class="blog-entry">
                            <div class="desc">
                                <h1 class="text-center"><strong>Sliced Score Matching: A Scalable Approach to Density and Score Estimation</strong></h1>
                                <h3 class="text-center">Yang Song, Sahaj Garg, Jiaxin Shi and Stefano Ermon</h3>
                                <h4 class="text-center">In <em>Uncertainty in Artificial Intelligence (UAI)</em>, 2020</h4>
                                <p class="text-center">score matching</p>
                            
                                <!--Bibtex citation-->
                                <style>
                                    body {
                                        margin: 0 auto; /* Remove default margin */
                                        background-color: #f0f0f0; /* Optional background color */
                                    }
                                    .bibtex-box {
                                        border: 1px solid #ccc;
                                        border-radius: 5px;
                                        padding: 5px; /* Reduced padding */
                                        margin: 20px auto; /* Center horizontally and add vertical margins */
                                        font-family: monospace;
                                        background-color: #f9f9f9;
                                        max-width: 600px; /* Set a maximum width */
                                        max-height: auto;
                                        text-align: left; /* Align text to the left */
                                    }
                                    .title {
                                        margin-bottom: 5px; /* 5px padding below the title */
                                    }
                                </style>
                                <div class="bibtex-box">
                                    <span style="font-size: smaller;" class="title">@inproceedings{song2020sliced,</span><br>
                                    <span style="padding-left: 20px;font-size: smaller;">title={Sliced score matching: A scalable approach to density and score estimation},</span><br>
                                    <span style="padding-left: 20px;font-size: smaller;">author={Song, Yang and Garg, Sahaj and Shi, Jiaxin and Ermon, Stefano},</span><br>
                                    <span style="padding-left: 20px;font-size: smaller;">booktitle={Uncertainty in Artificial Intelligence},</span><br>
                                    <span style="padding-left: 20px;font-size: smaller;">pages={574--584},</span><br>
                                    <span style="padding-left: 20px;font-size: smaller;">year={2020},</span><br>
                                    <span style="padding-left: 20px;font-size: smaller;">organization={PMLR}</span><br>
                                    <span>}</span>
                                </div>
                                <div class="text-center">
                                    <a href="https://proceedings.mlr.press/v115/song20a/song20a.pdf" class="btn btn-primary btn-outline">PDF</a>
                                    <a href="https://youtu.be/wMmqCMwuM2Q" class="btn btn-primary btn-outline">Video by Author</a>
                                </div>
                                <div style="padding-left: 20px;">
                                    <p>TL;DR: This paper introduces sliced score matching, a new method to compute the score function by projecting the score onto random vectors.</p>
                                </div>
                            </div>
                            <div class="row" style="margin-top: 50px;">
                                <div class="col-md-12">
                                    <div class="article animate-box">
                                        <h2 id="table-of-contents">
                                            Table of Contents
                                        </h2>
                                        <h4 style="padding-left: 20px;">
                                            <a href="#introduction" style="text-decoration: none; color: inherit;">1. Introduction</a>
                                        </h4>
                                        <h4 style="padding-left: 20px;">
                                            <a href="#score-functions-intuition" style="text-decoration: none; color: inherit;">2. Score Functions Intuition</a>
                                        </h4>
                                        <h4 style="padding-left: 20px;">
                                            <a href="#score-function" style="text-decoration: none; color: inherit;">3. Score Function: Bypassing the normalizing constant</a>
                                        </h4>
                                        <h4 style="padding-left: 20px;">
                                            <a href="#sliced-score-matching" style="text-decoration: none; color: inherit;">4. Sliced Score Matching</a>
                                        </h4>
                                        <h4 style="padding-left: 20px;">
                                            <a href="#previous-works" style="text-decoration: none; color: inherit;">5. Alternatives to Sliced Score Matching</a>
                                        </h4>
                                    </div>
                                </div>
                            </div>
                            <div class="row">
                                <div class="col-md-12">
                                    <div class="article animate-box">
                                        <h2 id="introduction">
                                            <a href="#table-of-contents" style="text-decoration: none; color: inherit;">1. Introduction</a>
                                        </h2>
                                        <p>
                                            The general goal of score matching is the estimation of the probability distibution of the data. In other words, we hope to find the parameters of a model so the model distribution is close to the data distribution.
                                            The model respresents the parametrized probability distribution of the data, which we call model distribution.
                                            <div style="text-align: center; margin: 0; padding: 0; margin-top: 10px; margin-bottom: 10px;">
                                                <img src="sliced-score-matching-a-scalable-approach-to-density-and-score-estimation/idea.png" 
                                                     alt="Score Matching Idea" 
                                                     style="width: 70%; height: auto; display: block; margin: 0 auto; padding: 0;">
                                                <p style="font-size: small; margin: 0; padding: 0;">Fig. 1. Generative modeling approach.</p>
                                            </div>
                                            Our dataset contains \( N \) samples and \( \mathbf{x}_i \) is each data point in the dataset. From the all the models with probability distributions \( \Theta \) we want to find a single probability distribution \( \theta \in \Theta \) by minimizing the distance between \( p_{data} \) and \( p_\theta \), so then we can generate samples from \( p_\theta \) <a>(Fig. 2)</a>.
                                            <div style="text-align: center; margin: 0; padding: 0; margin-top: 10px; margin-bottom: 10px;">
                                                <img src="sliced-score-matching-a-scalable-approach-to-density-and-score-estimation/training-generative-models.png" 
                                                     alt="Training Generative Models" 
                                                     style="width: 70%; height: auto; display: block; margin: 0 auto; padding: 0;">
                                                <p style="font-size: small; margin: 0; padding: 0;">Fig. 2. Generative modeling training.</p>
                                            </div>
                                            BUT the data distribution is very complex for high dimensional data.
                                            We will start from a gaussian distribution which is a graph with 2 layers, the data points and a single unit, which is the density function of the probability distribution of such points.
                                            <div style="text-align: center; margin: 0; padding: 0; margin-top: 10px; margin-bottom: 10px;">
                                                <img src="sliced-score-matching-a-scalable-approach-to-density-and-score-estimation/graph.png" 
                                                     alt="Graph" 
                                                     style="width: 30%; height: auto; display: block; margin: 0 auto; padding: 0;">
                                                <p style="font-size: small; margin: 0; padding: 0;">Fig. 2. Graph of 2 layers.</p>
                                            </div>
                                            This distribution is too simple to model high dimensional data, so we need to add more layers and build a deeper computational graph or neural network to model the probability distribution \( p_\theta \), where \( \theta \) denotes the weights of the network.
                                            <div style="text-align: center; margin: 0; padding: 0; margin-top: 10px; margin-bottom: 10px;">
                                                <img src="sliced-score-matching-a-scalable-approach-to-density-and-score-estimation/deep-graph.png" 
                                                     alt="Deep Graph" 
                                                     style="width: 30%; height: auto; display: block; margin: 0 auto; padding: 0;">
                                                <p style="font-size: small; margin: 0; padding: 0;">Fig. 3. Deep graph.</p>
                                            </div>
                                            The first step is to convert the output \( f_\theta(x) \) to its exponential \( e^{f_\theta(x)} \) so the output becomes positive. Then, we can normalize the output by dividing it by a constant \( Z_\theta \) in order to construct the probability distribution \( \frac{e^{f_\theta(x)}}{Z_\theta} = p_\theta \).
                                            <div style="text-align: center; margin: 0; padding: 0; margin-top: 10px; margin-bottom: 10px;">
                                                <img src="sliced-score-matching-a-scalable-approach-to-density-and-score-estimation/normalized-graph.png" 
                                                     alt="Noormalized Graph" 
                                                     style="width: 40%; height: auto; display: block; margin: 0 auto; padding: 0;">
                                                <p style="font-size: small; margin: 0; padding: 0;">Fig. 4. Normalized graph.</p>
                                            </div>
                                            By definition, the normilizing constant \( Z\theta \) is be computed as follows:
                                            \[
                                                Z_\theta = \int{e^{f_{\theta}(x)}} \, dx,
                                                \tag{Eq. 1} \label{eq:normalizing-constant}
                                            \]
                                            which, in gaussian distributions can be expressed as:
                                            \[
                                                Z_\mu = \frac{1}{(2\pi)^{d/2}},
                                                \tag{Eq. 2} \label{eq:normalizing-constant-gaussian}
                                            \]
                                            However, when handling bigger and more complex distributions, this constant is intractable to compute. So, how to tackle the intractable normalizing constant?
                                            Many approaches have been proposed but:
                                            <ul>
                                                <li>Energy-based models <a>[Ackley 1995, LeCun 2006]</a> \( \rightarrow \) Innacurate probability evaluation.</li>
                                                <li>Restricted neural network models \( \rightarrow \) Restricted model family:</li>
                                                <ul>
                                                    <li>Autoregressive models <a>[Bengio & Bengio 2000, van der Oord 2016]</a></li>
                                                    <li>Normalizing flows <a>[Rezende and Mohamed 2015], [Dinh 2014]</a></li>
                                                    <li>Variational autoencoders <a>[Kingma and Weilling 2014], [Rezende 2014]</a>.</li>
                                                </ul>
                                            </ul>
                                            So, instead of modeling the probability density function, the alternative is to model the data generating process, like:
                                            <ul>
                                                <li>Generative adversarial networks (GANS) <a>[Goodfellow 2014]</a></li>
                                            </ul>
                                            However, these approaches do not model underlyingmodel data distribution, they cannot provide accurate probability values. We need a network that takes full advantage of deep neaural networks, evaluate probability values accurately and improve the generation quality and provide controllability.
                                        </p>
                                    </div>
                                </div>
                            </div>
                            <div class="row">
                                <div class="col-md-12">
                                    <div class="article animate-box">
                                        <h2 id="score-functions-intuition">
                                            <a href="#table-of-contents" style="text-decoration: none; color: inherit;">2. Score Functions Intuition</a>
                                        </h2>
                                        <p>
                                            Suppose that we have a continuous probability distribution for which we represent \( p(\mathbf{x}) \) as the probability density function. We define the (Stein) score function as \( \nabla_x \text{log}p(x) \). 
                                            So, what is the difference between the socre function and the density function?
                                            If we show a mixture of 2 gaussians as in <a>Figure</a>, the density function is the color coded, where darker color indicates higher density, and the score function is a vector field which gives us the direction where the density function grows quicklier.
                                            <div style="text-align: center; margin: 0; padding: 0; margin-top: 10px; margin-bottom: 10px;">
                                                <img src="sliced-score-matching-a-scalable-approach-to-density-and-score-estimation/score-function.png" 
                                                     alt="Score Function" 
                                                     style="width: 20%; height: auto; display: block; margin: 0 auto; padding: 0;">
                                                <p style="font-size: small; margin: 0; padding: 0;">Fig. 5. Score function.</p>
                                            </div>
                                            So, given the score function we can compute the density function with the integral, and reversily with the derivative, which means that all the information is preserved.
                                            The models that work with the score function are called score-based generative models. Score-based models provide several advantages such as:
                                            <ul>
                                                <li>Flexible models: since the score function does not need to be normalized.</li>
                                                <li>Improved generation: higher sample quality han GANS and provide controllability.</li>
                                                <li>Probability evaluation: the probability evaluation is more accurate and also the estimation of data probabilities.</li>
                                            </ul>
                                            <div style="text-align: center; margin: 0; padding: 0; margin-top: 10px; margin-bottom: 10px;">
                                                <img src="sliced-score-matching-a-scalable-approach-to-density-and-score-estimation/overview.png" 
                                                     alt="Score Function" 
                                                     style="width: 70%; height: auto; display: block; margin: 0 auto; padding: 0;">
                                                <p style="font-size: small; margin: 0; padding: 0;">Fig. 6. Overview.</p>
                                            </div>
                                        </p>
                                    </div>
                                </div>
                            </div>
                            <div class="row">
                                <div class="col-md-12">
                                    <div class="article animate-box">
                                        <h2 id="score-function">
                                            <a href="#table-of-contents" style="text-decoration: none; color: inherit;">3. Score Function: Bypassing the normalizing constant</a>
                                        </h2>
                                        <p>
                                            If we want to model the probability distribution using a normalized robability model, we need to ensure that the distribution we want to represent is fully normalized, which means that the are under the density function has to be 1. But we always need to deal with the normalizing constant.
                                            <br>
                                            In contrast, if we model the distribution through the score functions, there is no such noralization restriction because wwhen we compute the score function of the neural network, the score function becomes the difference between two terms which second term is the gradient of the normalizing constant which is equal to zero. So, the score function becomes the gradient of the deep neural network. Such gradients are easy to cumpute with automatic differenciation and backpropagation.
                                            <div style="text-align: center; margin: 0; padding: 0; margin-top: 10px; margin-bottom: 10px;">
                                                <img src="sliced-score-matching-a-scalable-approach-to-density-and-score-estimation/score-function-graph.png" 
                                                     alt="Score Function" 
                                                     style="width: 60%; height: auto; display: block; margin: 0 auto; padding: 0;">
                                                <p style="font-size: small; margin: 0; padding: 0;">Fig. 7. Score function vs probability density function.</p>
                                            </div>
                                            Supposing we have a dataset with points and the underlying density given by \( p_{data} \): \( \{ \mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_N \} \overset{\text{i.i.d.}}{\sim} p_{\text{data}}(\mathbf{x}) \), we can train a statistical model to obtain the underlying density function \( p_\theta(\mathbf{x}) \approx p_{data}(\mathbf{x}) \) using methods such as maximum likelihood.
                                            If we work with score funtions, we want to develop a similar approach tthat can allow us to train a score model to estimate the underlying score function from a limited set of training data points. 
                                            We can formulate this problem as score estimation:
                                            <ul>
                                                <li>Given: \( \{ \mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_N \} \overset{\text{i.i.d.}}{\sim} p_{\text{data}}(\mathbf{x}) \)</li>
                                                <li>Goal: \( \nabla_x \text{log} p_{data}(\mathbf{x}) \)</li>
                                                <li>Score model (assumed to be a neural network): \( s_\theta : \mathbb{R}^d \rightarrow \mathbb{R}^d \approx \nabla_x \text{log} p_{data}(\mathbf{x}) \). It maps the deep dimensional input to a deep dimensional output. We want to train thie score model so it approximates the ground truth score function of the data distribution \( \nabla_x \text{log} p_{data}(\mathbf{x}) \).</li>
                                                <li>Objective: How to train such model? By comparing two vector fields of scores:</li>
                                                <div style="text-align: center; margin: 0; padding: 0; margin-top: 10px; margin-bottom: 10px;">
                                                    <img src="sliced-score-matching-a-scalable-approach-to-density-and-score-estimation/vector-fields.png" 
                                                         alt="Score Function" 
                                                         style="width: 60%; height: auto; display: block; margin: 0 auto; padding: 0;">
                                                    <p style="font-size: small; margin: 0; padding: 0;">Fig. 8. Vector fields.</p>
                                                </div>
                                                <li>Prediction (by our score model): \( s_\theta(\mathbf{x}) \).</li>
                                                <li>Ground truth: \( \nabla_x \text{log} p_{data}(\mathbf{x}) \).</li>
                                            </ul>
                                            We might be able to compute the difference vectors between the pairs of vectors from the original vector fields <a>(Fig. 9)</a>.
                                            <div style="text-align: center; margin: 0; padding: 0; margin-top: 10px; margin-bottom: 10px;">
                                                <img src="sliced-score-matching-a-scalable-approach-to-density-and-score-estimation/vector-field-difference.png" 
                                                     alt="Difference Vectors" 
                                                     style="width: 60%; height: auto; display: block; margin: 0 auto; padding: 0;">
                                                <p style="font-size: small; margin: 0; padding: 0;">Fig. 9. Vector field differences that form an objective to optimize.</p>
                                            </div>
                                            Mathematically, this calculation can be done with Fisher divergence:
                                            \[
                                            \frac{1}{2} \mathbb{E}_{p_{\text{data}}(\mathbf{x})} \left[ \left\| \nabla \log p_{\text{data}}(\mathbf{x}) - s_\theta(\mathbf{x}) \right\|^2_2 \right]
                                                \tag{Eq. 3} \label{eq:fisher-divergence}
                                            \]
                                            However, we cannot compute directly Fishe's divergence because we do not know the ground truth value of the data score function. Therefore, we will set the objective with Score Matching <a>[Hyvärinen 2005]</a>:
                                            \[
                                            \mathbb{E}_{p_{\text{data}}(\mathbf{x})} \left[ \frac{1}{2}   \left\| s_\theta(\mathbf{x}) \right\|^2_2 + \text{trace}(\nabla_x s_\theta(\mathbf{x})) \right],
                                                \tag{Eq. 4} \label{eq:score-matching}
                                            \]
                                            where:
                                            <ul>
                                                <li>\( \nabla_x s_\theta(\mathbf{x}) \) is the Jacobian of \( s_\theta(\mathbf{x}) \).</li>
                                            </ul>
                                            The expectation \( \mathbb{E}_{p_\text{data}(\mathbf{x})} \) can be approximated by the mean over the dataset samples: 
                                            \[
                                                \mathbb{E}_{p_{\text{data}}(\mathbf{x})} \left[ \frac{1}{2}   \left\| s_\theta(\mathbf{x}) \right\|^2_2 + \text{trace}(\nabla_x s_\theta(\mathbf{x})) \right] = \frac{1}{N} \sum_{i=1}^{N} \left[ \frac{1}{2}   \left\| s_\theta(\mathbf{x_i}) \right\|^2_2 + \text{trace}(\nabla_x s_\theta(\mathbf{x_i})) \right].
                                                \tag{Eq. 5} \label{eq:score-matching-approximation}
                                            \]
                                            However, the score matching objective is not scalable to compute, especially when using deep neural networks.
                                            <br>
                                            Let's suppose our score is parametrized by a deep neural network which we cal deep score models. In order to compute the score matching, we need to compute two terms, one is the squared Euclidean norm of the score model output \( \left\| s_\theta(\mathbf{x_i}) \right\|^2_2 \) and the second term is the trace of the Jacobian of the score model \( \text{trace}(\nabla_x s_\theta(\mathbf{x_i})) \).
                                            The first term is easy to compute through forward propagation and them computing the L2 norm of the obtained \( s_\theta(\mathbf{x}) \). The second term is more difficult to compute since we need to forward propagate to obtain the first element and then we need to backpropagate to compute the first element on the diagonal of the Jacobian \( \frac{\partial s_\theta^1}{\partial x_1} \):
                                            \[
                                                J = \nabla_x s_\theta(\mathbf{x}) =
                                                \begin{bmatrix}
                                                \frac{\partial s_\theta^1}{\partial x_1} & \frac{\partial s_\theta^1}{\partial x_2} & \cdots & \frac{\partial s_\theta^1}{\partial x_d} \\
                                                \frac{\partial s_\theta^2}{\partial x_1} & \frac{\partial s_\theta^2}{\partial x_2} & \cdots & \frac{\partial s_\theta^2}{\partial x_d} \\
                                                \vdots & \vdots & \ddots & \vdots \\
                                                \frac{\partial s_\theta^N}{\partial x_1} & \frac{\partial s_\theta^N}{\partial x_2} & \cdots & \frac{\partial s_\theta^N}{\partial x_d}
                                                \end{bmatrix}.
                                                \tag{Eq. 6} \label{eq:jacobian}
                                            \]
                                            This process needs to be repeated multiple times until we have recovered all diagonal elements and then we can sum over all the diagonal elements to get the trace <a>(Fig. 10)</a>:
                                            <div style="text-align: center; margin: 0; padding: 0; margin-top: 10px; margin-bottom: 10px;">
                                                <img src="sliced-score-matching-a-scalable-approach-to-density-and-score-estimation/jacobian.png" 
                                                     alt="Jacobian" 
                                                     style="width: 40%; height: auto; display: block; margin: 0 auto; padding: 0;">
                                                <p style="font-size: small; margin: 0; padding: 0;">Fig. 10. Jacobian computation.</p>
                                            </div>
                                            This process consists of lots of backpropagations, which is proportional to the dimensionality of our data points, which makes score matching non scalable.
                                            To avoid this and improve scalability, this paper proposes <strong>sliced score matching</strong>.
                                        </p>
                                    </div>
                                </div>
                            </div>

                            <div class="row">
                                <div class="col-md-12">
                                    <div class="article animate-box">
                                        <h2 id="sliced-score-matching">
                                            <a href="#table-of-contents" style="text-decoration: none; color: inherit;">4. Sliced Score Matching</a>
                                        </h2>
                                        <p>
                                            Intuition: one dimensional problems should be easier to compute, so how can we convert a high-dimensional problem into a one-dimensional problem?
                                            <br>
                                            Idea: By leveraging random projecctions. Projecting high-dimensional vector fields to run directions, will allow us to get one-dimensional scalar fields.
                                            <br>
                                            Suppose that we have the following high-dimensional vectors in <a>Fig. 11</a>:
                                            <div style="text-align: center; margin: 0; padding: 0; margin-top: 10px; margin-bottom: 10px;">
                                                <img src="sliced-score-matching-a-scalable-approach-to-density-and-score-estimation/high-dimensional-vectors.png" 
                                                     alt="High Dimensional Vectors" 
                                                     style="width: 40%; height: auto; display: block; margin: 0 auto; padding: 0;">
                                                <p style="font-size: small; margin: 0; padding: 0;">Fig. 11. High-dimensional vectors.</p>
                                            </div>
                                            we can project these vectors onto a random direction \( \mathbf{v} \) to get the one-dimensional scalar fields in <a>Fig. 12</a>:
                                            <div style="text-align: center; margin: 0; padding: 0; margin-top: 10px; margin-bottom: 10px;">
                                                <img src="sliced-score-matching-a-scalable-approach-to-density-and-score-estimation/one-dimensional-scalar-fields.png" 
                                                     alt="One Dimensional Scalar Fields" 
                                                     style="width: 40%; height: auto; display: block; margin: 0 auto; padding: 0;">
                                                <p style="font-size: small; margin: 0; padding: 0;">Fig. 12. One-dimensional scalar fields.</p>
                                            </div>
                                            This intuition can be captured with the concept of sliced Fisher divergence:
                                            \[
                                                \frac{1}{2} \mathbb{E}_{p_v}  \mathbb{E}_{p_{\text{data}}(\mathbf{x})} \left[ ( \mathbf{v}^\text{T} \nabla_x \log p_{\text{data}}(\mathbf{x}) - \mathbf{v}^\text{T}s_\theta(\mathbf{x}) )^2 \right],
                                                \tag{Eq. 7} \label{eq:sliced-fisher-divergence}
                                            \]
                                            where:
                                            <ul>
                                                <li>\( p_v \) is the distribution of the random direction \( \mathbf{v} \).</li>
                                                <li>\( \mathbf{v}^\text{T} \nabla_x \log p_{\text{data}}(\mathbf{x}) \) is the projection of the ground truth score function onto the random direction \( \mathbf{v} \).</li>
                                                <li>\( \mathbf{v}^\text{T}s_\theta(\mathbf{x}) \) is the projection of the score model onto the random direction \( \mathbf{v} \).</li>
                                                <li> denotes the distribution of those projection directions.</li>
                                            </ul>
                                            Again, we can leverage integration by parts to eliminate the dependency on the ground truth data score:
                                            \[
                                            \frac{1}{2} \mathbb{E}_{p_v}  \mathbb{E}_{p_{\text{data}}(\mathbf{x})} \left[ \mathbf{v}^\text{T} \nabla_x s_\theta(\mathbf{x}) \mathbf{v} + \frac{1}{2} (\mathbf{v}^\text{T}s_\theta(\mathbf{x}) )^2 \right],
                                                \tag{Eq. 8} \label{eq:sliced-fisher-divergence-integration-by-parts}
                                            \]
                                            Since \( \mathbf{v}^\text{T} \nabla_x s_\theta(\mathbf{x}) \mathbf{v} \) is tractable, there is no trace of the Jacobian but the product of vector \( \mathbf{v}^\text{T} \), Jacovian \( \nabla_x s_\theta(\mathbf{x}) \) and vector \( \mathbf{v} \), which is scalable.
                                            We can rewrite the term as:
                                            \[
                                            \nabla_x s_\theta(\mathbf{x}) \mathbf{v} = \mathbf{v}^\text{T} \nabla_x (\mathbf{v}^\text{T} s_\theta(\mathbf{x})).
                                                \tag{Eq. 9} \label{eq:trace-v-vt-jacobian}
                                            \]
                                            To compute it, we need to compute \( s_\theta(\mathbf{x}) \) by forward propagation. Then, we can compute the inner product between $\mathbf{v}^\text{T}$ and \( s_\theta(\mathbf{x}) \), which only required the addition of one neuron to the computational graph <a>(Fig. 13)</a>.
                                            <div style="text-align: center; margin: 0; padding: 0; margin-top: 10px; margin-bottom: 10px;">
                                                <img src="sliced-score-matching-a-scalable-approach-to-density-and-score-estimation/trace-v-vt-jacobian.png" 
                                                     alt="Trace v vT Jacobian"
                                                        style="width: 40%; height: auto; display: block; margin: 0 auto; padding: 0;">
                                                <p style="font-size: small; margin: 0; padding: 0;">Fig. 13. Trace \( \mathbf{v}^\text{T} \nabla_x s_\theta(\mathbf{x}) \mathbf{v} \) computation.</p>
                                            </div>
                                            Next, we can compute the gradient \( \nabla_x (\mathbf{v}^\text{T} s_\theta(\mathbf{x})) \) by doing only one backpropagation, and at the end, compute the inner prodect of \( \mathbf{v}^\text{T} \) and the gradient <a>(Fig. 14)</a>
                                            <div style="text-align: center; margin: 0; padding: 0; margin-top: 10px; margin-bottom: 10px;">
                                                <img src="sliced-score-matching-a-scalable-approach-to-density-and-score-estimation/trace-v-vt-jacobian-backpropagation.png" 
                                                     alt="Trace v vT Jacobian Backpropagation"
                                                        style="width: 50%; height: auto; display: block; margin: 0 auto; padding: 0;">
                                                <p style="font-size: small; margin: 0; padding: 0;">Fig. 14. Trace \( \mathbf{v}^\text{T} \nabla_x s_\theta(\mathbf{x}) \mathbf{v} \) backpropagation.</p>
                                            </div>
                                            In summary, we only need one step backpropagation compared to the multiple backpropagation steps in vanilla score matching.
                                            <br>
                                            In practice, we can summarize sliced score matching as:
                                            <ul>
                                                <li>Sample a minibatch of datapoints from our dataset \( \{ \mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_n \} \overset{}{\sim} p_{\text{data}}(\mathbf{x}) \).</li>
                                                <li>Sample a minibatch of projection directions \( \{ \mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}n \} \overset{}{\sim} p_n \) (for each datapoint, sample one single direction from the distribution \( p_v \) ).</li>
                                                <li>Estimate the sliced score matching loss with emirical means:</li>
                                                \[
                                                    \frac{1}{n} \sum_{i=1}^n  \left[ \mathbf{v}^\text{T} \nabla_x s_\theta(\mathbf{x_i}) \mathbf{v_i} + \frac{1}{2} (\mathbf{v_i}^\text{T}s_\theta(\mathbf{x_i}) )^2 \right].
                                                    \tag{Eq. 10} \label{eq:sliced-score-matching-loss}
                                                \]
                                                <li>The projection distribution is typically Gaussian or Rademacher.</li>
                                                <li>Stochastic gradient descend → minimaze the empirical objective for sliced score matching.</li>
                                            </ul>
                                        </p>
                                    </div>
                                </div>
                            </div>

                            <div class="row">
                                <div class="col-md-12">
                                    <div class="article animate-box">
                                        <h2 id="previous-works">
                                            <a href="#table-of-contents" style="text-decoration: none; color: inherit;">5. Alternatives to Sliced Score Matching: Previous works</a>
                                        </h2>
                                        <h3 id="previous-works-vincent">
                                            <a href="#table-of-contents" style="text-decoration: none; color: inherit;">5.1 Denoising Score Matching [Vincent 2011]</a>
                                        </h3>
                                        There is an alternative to bypass the computational challenge of vanilla score matching with <strong>denoising score matching</strong>.
                                        <br>
                                        Denoising score matching takes its name from the procedure to solve score matching scalability. It consists of adding noise to the data point to help us computing the trace of the Jacobian term.
                                        To peform denoising score matching we need to define a perturbation kernel denoted as \( q_\sigma \).
                                        <br>
                                        Being \( \mathbf{x} \) a noise-free datapoint, \( \mathbf{\tilde{x}} \) a perturbed data point and \( \sigma \) a gaussian distribution (typically), we convolve the perturbation kernel with the original data distribution \( p_{\text{data}}(\mathbf{x}) \), to get a noisy data distribution \( q_\sigma (\mathbf{\tilde{x}}) \).
                                        The key idea behind denoising score matching is to estimate the scope function of the noise data density instead of the score function of the original data density.
                                        \[
                                            \frac{1}{2} \mathbb{E}_{q_\sigma(\mathbf{\tilde{x}})} \left[ \left\| \nabla_\mathbf{\tilde{x}} \log p_\sigma(\mathbf{\tilde{x}}) - s_\theta(\mathbf{\tilde{x}}) \right\|^2_2 \right].
                                            \tag{Eq. 11} \label{eq:denoising-score-matching}
                                        \]
                                        When estimating the score function of the noisy data distribution, the equivalent form obtained sfter aritmetic deviation which is the objective of denoising score matching is:
                                        \[
                                        \frac{1}{2} \mathbb{E}_{p_{\text{data}(\mathbf{x})}} \mathbb{E}_{q_\sigma (\mathbf{\tilde{x} \vert \mathbf{x}}}) \left[ \left\| \nabla_\mathbf{\tilde{x}}  \log p_\sigma(\mathbf{\tilde{x}} \vert \mathbf{x}) - s_\theta(\mathbf{\tilde{x}}) \right\|^2_2 \right].
                                            \tag{Eq. 12} \label{eq:denoising-score-matching-objective}
                                        \]
                                        The gradient of the perturbation kernel $\nabla_\mathbf{\tilde{x}}  \log p_\sigma(\mathbf{\tilde{x}} \vert \mathbf{x})$ is scalable and fully tractable because we usually define the perturbation kernel by hand.
                                        <div style="text-align: center; margin: 0; padding: 0; margin-top: 10px; margin-bottom: 10px;">
                                            <img src="sliced-score-matching-a-scalable-approach-to-density-and-score-estimation/score-matching-objectives.png" 
                                                 alt="Trace v vT Jacobian Backpropagation"
                                                    style="width: 70%; height: auto; display: block; margin: 0 auto; padding: 0;">
                                            <p style="font-size: small; margin: 0; padding: 0;">Fig. 15. Score matching objectives.</p>
                                        </div>
                                        The tradeoff of denoising score matching is that, since it requires adding noise to datapoints, it cannot estimate the scores of the noise-free distributions. When are trying to lower the magnitude of the noise, the variance becomes bigger and bigger and eventually explodes. Therefore, it is not to use denoising score matching for noise-free score estimation.
                                        <br>
                                        Applying first the definition of expectation in continuous probability and then expandinf the square of the norm \( \| \mathbf{a} - \mathbf{b} \|_2^2 = \| \mathbf{a} \|_2^2 + \| \mathbf{b} \|_2^2 - 2 \mathbf{a}^\top \mathbf{b} \), we can derive the objective:
                                        \[
                                            \begin{align}
                                            \frac{1}{2} \mathbb{E}_{\tilde{\mathbf{x}} \sim q_\sigma} \left[ \left\| \nabla_{\tilde{\mathbf{x}}} \log q_\sigma(\tilde{\mathbf{x}}) - s_\theta(\tilde{\mathbf{x}}) \right\|_2^2 \right] &= \tag{Eq. 13.1} \\
                                            &= \frac{1}{2} \int q_\sigma(\tilde{\mathbf{x}}) \left\| \nabla_{\tilde{\mathbf{x}}} \log q_\sigma(\tilde{\mathbf{x}}) - s_\theta(\tilde{\mathbf{x}}) \right\|_2^2 d\tilde{\mathbf{x}} \tag{Eq. 13.2} \\
                                            &= \frac{1}{2} \int q_\sigma(\tilde{\mathbf{x}}) \left\| \nabla_{\tilde{\mathbf{x}}} \log q_\sigma(\tilde{\mathbf{x}}) \right\|_2^2 d\tilde{\mathbf{x}} 
                                            + \frac{1}{2} \int q_\sigma(\tilde{\mathbf{x}}) \left\| s_\theta(\tilde{\mathbf{x}}) \right\|_2^2 d\tilde{\mathbf{x}} 
                                            - \int q_\sigma(\tilde{\mathbf{x}}) \nabla_{\tilde{\mathbf{x}}} \log q_\sigma(\tilde{\mathbf{x}})^\top s_\theta(\tilde{\mathbf{x}}) d\tilde{\mathbf{x}}. \tag{Eq. 13.3}
                                            \end{align}
                                            \label{eq:denoising-score-matching-objective-expanded}
                                        \]
                                        where we can write the third term as an expectation:
                                        \[
                                            \begin{align}
                                            - \int q_\sigma(\tilde{\mathbf{x}}) \nabla_{\tilde{\mathbf{x}}} \log q_\sigma(\tilde{\mathbf{x}})^\top \mathbf{s}_\theta(\tilde{\mathbf{x}}) \, d\tilde{\mathbf{x}} \tag{Eq. 14.1} \\
                                            &= - \int \nabla_{\tilde{\mathbf{x}}} q_\sigma(\tilde{\mathbf{x}})^\top \mathbf{s}_\theta(\tilde{\mathbf{x}}) \, d\tilde{\mathbf{x}} \tag{Eq. 14.2} \\
                                            &= - \int \nabla_{\tilde{\mathbf{x}}} \left( \int p_\text{data}(\mathbf{x}) q_\sigma(\tilde{\mathbf{x}} | \mathbf{x}) \, d\mathbf{x} \right)^\top \mathbf{s}_\theta(\tilde{\mathbf{x}}) \, d\tilde{\mathbf{x}} \tag{Eq. 14.3} \\
                                            &= - \int \nabla_{\tilde{\mathbf{x}}} \left( \int p_\text{data}(\mathbf{x}) q_\sigma(\tilde{\mathbf{x}} | \mathbf{x}) \, d\mathbf{x} \right)^\top \mathbf{s}_\theta(\tilde{\mathbf{x}}) \, d\tilde{\mathbf{x}} \tag{Eq. 14.4} \\
                                            &= - \int \left( \int p_\text{data}(\mathbf{x}) \nabla_{\tilde{\mathbf{x}}} q_\sigma(\tilde{\mathbf{x}} | \mathbf{x}) \, d\mathbf{x} \right)^\top \mathbf{s}_\theta(\tilde{\mathbf{x}}) \, d\tilde{\mathbf{x}} \tag{Eq. 14.5} \\
                                            &= - \int \left( \int p_\text{data}(\mathbf{x}) \nabla_{\tilde{\mathbf{x}}} q_\sigma(\tilde{\mathbf{x}} | \mathbf{x}) \, d\mathbf{x} \right)^\top \mathbf{s}_\theta(\tilde{\mathbf{x}}) \, d\tilde{\mathbf{x}} \tag{Eq. 14.6} \\
                                            &= - \int \left( \int p_\text{data}(\mathbf{x}) q_\sigma(\tilde{\mathbf{x}} | \mathbf{x}) \nabla_{\tilde{\mathbf{x}}} \log q_\sigma(\tilde{\mathbf{x}} | \mathbf{x}) \, d\mathbf{x} \right)^\top \mathbf{s}_\theta(\tilde{\mathbf{x}}) \, d\tilde{\mathbf{x}} \tag{Eq. 14.7} \\
                                            &= - \int \left( \int p_\text{data}(\mathbf{x}) q_\sigma(\tilde{\mathbf{x}} | \mathbf{x}) \nabla_{\tilde{\mathbf{x}}} \log q_\sigma(\tilde{\mathbf{x}} | \mathbf{x}) \, d\mathbf{x} \right)^\top \mathbf{s}_\theta(\tilde{\mathbf{x}}) \, d\tilde{\mathbf{x}} \tag{Eq. 14.8} \\
                                            &= - \int \int p_\text{data}(\mathbf{x}) q_\sigma(\tilde{\mathbf{x}} | \mathbf{x}) \nabla_{\tilde{\mathbf{x}}} \log q_\sigma(\tilde{\mathbf{x}} | \mathbf{x})^\top \mathbf{s}_\theta(\tilde{\mathbf{x}}) \, d\mathbf{x} \, d\tilde{\mathbf{x}} \tag{Eq. 14.9} \\
                                            &= - \int \int p_\text{data}(\mathbf{x}) q_\sigma(\tilde{\mathbf{x}} | \mathbf{x}) \nabla_{\tilde{\mathbf{x}}} \log q_\sigma(\tilde{\mathbf{x}} | \mathbf{x})^\top \mathbf{s}_\theta(\tilde{\mathbf{x}}) \, d\mathbf{x} \, d\tilde{\mathbf{x}} \tag{Eq. 14.10} \\
                                            &= - \mathbb{E}_{\mathbf{x} \sim p_\text{data}(\mathbf{x}), \tilde{\mathbf{x}} \sim q_\sigma(\tilde{\mathbf{x}} | \mathbf{x})} \left[ \nabla_{\tilde{\mathbf{x}}} \log q_\sigma(\tilde{\mathbf{x}} | \mathbf{x})^\top \mathbf{s}_\theta(\tilde{\mathbf{x}}) \right] \tag{Eq. 14.11} \\
         
                                            \end{align}
                                        \]
                                        with this derivation, we can rewrite <a>Eq. 13.1.</a> as:
                                        \[
                                            \begin{align}
                                            \frac{1}{2} \mathbb{E}_{\tilde{\mathbf{x}} \sim q_\sigma} \left[ \left\| \nabla_{\tilde{\mathbf{x}}} \log q_\sigma(\tilde{\mathbf{x}}) - s_\theta(\tilde{\mathbf{x}}) \right\|_2^2 \right] \tag{Eq. 15.1} \\
                                            &= \text{const.} + \frac{1}{2} \mathbb{E}_{\tilde{\mathbf{x}} \sim q_\sigma} \left[ \| \mathbf{s}_\theta(\tilde{\mathbf{x}}) \|_2^2 \right] - \int q_\sigma(\tilde{\mathbf{x}}) \nabla_{\tilde{\mathbf{x}}} \log q_\sigma(\tilde{\mathbf{x}})^\top \mathbf{s}_\theta(\tilde{\mathbf{x}}) \, d\tilde{\mathbf{x}} \tag{Eq. 15.2} \\
                                            &= \text{const.} + \frac{1}{2} \mathbb{E}_{\tilde{\mathbf{x}} \sim q_\sigma} \left[ \| \mathbf{s}_\theta(\tilde{\mathbf{x}}) \|_2^2 \right] - \mathbb{E}_{\mathbf{x} \sim p_\text{data}(\mathbf{x}), \tilde{\mathbf{x}} \sim q_\sigma(\tilde{\mathbf{x}} | \mathbf{x})} \left[ \nabla_{\tilde{\mathbf{x}}} \log q_\sigma(\tilde{\mathbf{x}} | \mathbf{x})^\top \mathbf{s}_\theta(\tilde{\mathbf{x}}) \right] \tag{Eq. 15.3} \\
                                            \end{align}
                                        \]
                                        Introducing Squared Norm for the Gradient Term (3rd term) we get:
                                        \[
                                            \begin{align}
                                            = \text{const.} + \frac{1}{2} \mathbb{E}_{\mathbf{x} \sim p_\text{data}(\mathbf{x}), \tilde{\mathbf{x}} \sim q_\sigma(\tilde{\mathbf{x}} | \mathbf{x})} \left[ \| \mathbf{s}_\theta(\tilde{\mathbf{x}}) \|_2^2 \right] - \frac{1}{2} \mathbb{E}_{\mathbf{x} \sim p_\text{data}(\mathbf{x}), \tilde{\mathbf{x}} \sim q_\sigma(\tilde{\mathbf{x}} | \mathbf{x})} \left[ \| \nabla_{\tilde{\mathbf{x}}} \log q_\sigma(\tilde{\mathbf{x}} | \mathbf{x}) \|_2^2 \right] \tag{Eq. 16.1} \\
                                            = \text{const.} + \frac{1}{2} \mathbb{E}_{\mathbf{x} \sim p_\text{data}(\mathbf{x}), \tilde{\mathbf{x}} \sim q_\sigma(\tilde{\mathbf{x}} | \mathbf{x})} \left[ \| \mathbf{s}_\theta(\tilde{\mathbf{x}}) - \nabla_{\tilde{\mathbf{x}}} \log q_\sigma(\tilde{\mathbf{x}} | \mathbf{x}) \|_2^2 \right] + \text{const.} \tag{Eq. 16.2} \\
                                            = \frac{1}{2} \mathbb{E}_{\mathbf{x} \sim p_\text{data}(\mathbf{x}), \tilde{\mathbf{x}} \sim q_\sigma(\tilde{\mathbf{x}} | \mathbf{x})} \left[ \| \mathbf{s}_\theta(\tilde{\mathbf{x}}) - \nabla_{\tilde{\mathbf{x}}} \log q_\sigma(\tilde{\mathbf{x}} | \mathbf{x}) \|_2^2 \right] + \text{const.} \tag{Eq. 16.3} \\
                                            \end{align}
                                        \]
                                        As a conclusion, denoising score matching can be summarize as:
                                        <ul>
                                            <li>Sample a minibatch of datapoints: \( \{ \mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_N \} \sim p_{\text{data}}(\mathbf{x}) \)</li>
                                            <li>Sample a minibatch of perturbed datapoints: \( \{ \mathbf{\tilde{x}}_1, \mathbf{\tilde{x}}_2, \dots, \mathbf{\tilde{x}}n \} \overset{} {\sim} q_\sigma(\mathbf{\tilde{x}}) \) with \( \mathbf{\tilde{x}}_i \sim q_\sigma (\mathbf{\tilde{x}}_i \vert \mathbf{x}) \)</li>
                                            <li>Estimate the denoising score matching loss with empirical means:</li>
                                            \[
                                                = \frac{1}{2n} \sum_{i=1}^n \left[ \| \mathbf{s}_\theta(\tilde{\mathbf{x_i}}) - \nabla_{\tilde{\mathbf{x}}} \log q_\sigma(\tilde{\mathbf{x_i}} | \mathbf{x_i}) \|_2^2 \right].
                                                \tag{Eq. 17} \label{eq:denoising-score-matching-loss}
                                            \]
                                            In the special case of a Gaussian perturbation:
                                            \[
                                                = \frac{1}{2n} \sum_{i=1}^n \left[ \left| \left| \mathbf{s}_\theta(\tilde{\mathbf{x_i}}) - \frac{\tilde{\mathbf{x_i}} - \mathbf{x_i}}{\sigma^2} \right|\right|_2^2 \right].
                                                \tag{Eq. 18} \label{eq:gaussian-perturbation}
                                            \]
                                            <li>Stochastic gradient descent.</li>
                                            <li>Choose a very small \( \sigma \) (be careful because the variance can explode).</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

    


        <div class="row">
            <div class="col-md-12 text-center">
                <p>
                    &copy;
Copyright &copy;<script>document.write(new Date().getFullYear());</script> All rights reserved | Carlos Hern&aacute;ndez Oliv&aacute;n | <a href="https://colorlib.com" target="_blank">Colorlib</a>
                </p>
            </div>
        </div>
    </div>
</div>
</footer>

</div>

<!-- jQuery -->
<script src="../js/jquery.min.js"></script>
<!-- jQuery Easing -->
<script src="../js/jquery.easing.1.3.js"></script>
<!-- Bootstrap -->
<script src="../js/bootstrap.min.js"></script>
<!-- Waypoints -->
<script src="../js/jquery.waypoints.min.js"></script>
<!-- Owl Carousel -->
<script src="../js/owl.carousel.min.js"></script>
<!-- Magnific Popup -->
<script src="../js/jquery.magnific-popup.min.js"></script>
<script src="../js/magnific-popup-options.js"></script>

<!-- Main JS (Do not remove) -->
<script src="../js/main.js"></script>

</body>
</html>