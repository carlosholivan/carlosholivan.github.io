<!DOCTYPE HTML>
<html>
	<head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Carlos Hern&aacute;ndez Oliv&aacute;n</title>
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="description" content="" />
	<meta name="keywords" content="" />
	<meta name="author" content="" />

  <!-- Facebook and Twitter integration -->
	<meta property="og:title" content=""/>
	<meta property="og:image" content=""/>
	<meta property="og:url" content=""/>
	<meta property="og:site_name" content=""/>
	<meta property="og:description" content=""/>
	<meta name="twitter:title" content="" />
	<meta name="twitter:image" content="" />
	<meta name="twitter:url" content="" />
	<meta name="twitter:card" content="" />

	<!-- Place favicon.ico and apple-touch-icon.png in the root directory -->
	<link rel="shortcut icon" href="favicon.ico">

	<link href="https://fonts.googleapis.com/css?family=Karla:400,700" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Playfair+Display:400,400i,700" rel="stylesheet">
	
	<!-- Animate.css -->
	<link rel="stylesheet" href="../css/animate.css">
	<!-- Icomoon Icon Fonts-->
	<link rel="stylesheet" href="../css/icomoon.css">
	<!-- Bootstrap  -->
	<link rel="stylesheet" href="../css/bootstrap.css">
	<!-- Owl Carousel -->
	<link rel="stylesheet" href="../css/owl.carousel.min.css">
	<link rel="stylesheet" href="../css/owl.theme.default.min.css">
	<!-- Magnific Popup -->
	<link rel="stylesheet" href="../css/magnific-popup.css">

	<link rel="stylesheet" href="../css/style.css">


	<!-- Modernizr JS -->
	<script src="js/modernizr-2.6.2.min.js"></script>
	<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
	<!-- FOR IE9 below -->
	<!--[if lt IE 9]>
	<script src="js/respond.min.js"></script>
	<![endif]-->

    

	</head>
	<body>

	<!-- Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=G-3CDKKZRCVT"></script>
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());
		gtag('config', 'G-3CDKKZRCVT');
	</script>

	<nav id="colorlib-main-nav" role="navigation">
		<a href="#" class="js-colorlib-nav-toggle colorlib-nav-toggle active"><i></i></a>
		<div class="js-fullheight colorlib-table">
			<div class="colorlib-table-cell js-fullheight">
				<div class="row">
					<div class="col-md-12">
						<div class="form-group">
							<input type="text" class="form-control" id="search" placeholder="Enter any key to search...">
							<button type="submit" class="btn btn-primary"><i class="icon-search3"></i></button>
						</div>
					</div>
				</div>
				<div class="row">
					<div class="col-md-12">
						<ul>
							<li><a href="../index.html">Home</a></li>
							<li><a href="../curriculum.html">CV</a></li>
							<li><a href="../publications.html">Publications</a></li>
							<li><a href="../talks.html">Talks</a></li>
							<li class="active"><a href="../teaching.html">Teaching</a></li>
							<li><a href="../community-service.html">Community Service</a></li>
							<li><a href="../blog.html">Blog</a></li>
							<li><a href="../others.html">Others</a></li>
							<li><a href="../about.html">About</a></li>
							<li><a href="../contact.html">Contact</a></li>
						</ul>
					</div>
				</div>
			</div>
		</div>
	</nav>
	
	<div id="colorlib-page">
		<header>
			<div class="container">
				<div class="row">
					<div class="col-md-12">
						<div class="colorlib-navbar-brand">
							<a class="colorlib-logo" href="../index.html"><span>Ch</span><span>o</span></a>
						</div>
						<a href="#" class="js-colorlib-nav-toggle colorlib-nav-toggle"><i></i></a>
					</div>
				</div>
			</div>
		</header>

        <div id="colorlib-blog">
            <div class="container">
                <div class="row text-center">
                    <h2 class="bold">Paper Title</h2>
                </div>
                <div class="row">
                    <div class="col-md-12">
                        <div class="blog-entry">
                            <div class="desc">
                                <h1 class="text-center"><strong>High-Resolution Image Synthesis with Latent Diffusion Models</strong></h1>
                                <h3 class="text-center">Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser and Bj√∂rn Ommer</h3>
                                <h4 class="text-center">In <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2023</h4>
                                <p class="text-center">diffusion models, latent diffusion, VQ-VAE</p>
                            
                                <!--Bibtex citation-->
                                <style>
                                    body {
                                        margin: 0 auto; /* Remove default margin */
                                        background-color: #f0f0f0; /* Optional background color */
                                    }
                                    .bibtex-box {
                                        border: 1px solid #ccc;
                                        border-radius: 5px;
                                        padding: 5px; /* Reduced padding */
                                        margin: 20px auto; /* Center horizontally and add vertical margins */
                                        font-family: monospace;
                                        background-color: #f9f9f9;
                                        max-width: 600px; /* Set a maximum width */
                                        max-height: auto;
                                        text-align: left; /* Align text to the left */
                                    }
                                    .title {
                                        margin-bottom: 5px; /* 5px padding below the title */
                                    }
                                </style>
                                <div class="bibtex-box">
                                    <span style="font-size: smaller;" class="title">@inproceedings{Rombach2022latent,</span><br>
                                    <span style="padding-left: 20px;font-size: smaller;">title={High-Resolution Image Synthesis with Latent Diffusion Models},</span><br>
                                    <span style="padding-left: 20px;font-size: smaller;">author={Robin Rombach andAndreas Blattmann and Dominik Lorenz and Patrick Esser and Bj{\"{o}}rn Ommer},</span><br>
                                    <span style="padding-left: 20px;font-size: smaller;">booktitle={{IEEE/CVF} Conference on Computer Vision and Pattern Recognition, {CVPR}}</span><br>
                                    <span style="padding-left: 20px;font-size: smaller;">year={2023},</span><br>
                                    <span style="padding-left: 20px;font-size: smaller;">pages={10674--10685},</span><br>
                                    <span style="padding-left: 20px;font-size: smaller;">publisher={{IEEE}},</span><br>
                                    <span>}</span>
                                </div>
                                <div class="text-center">
                                    <a href="https://arxiv.org/abs/2112.10752" class="btn btn-primary btn-outline">PDF</a>
                                    <a href="https://github.com/CompVis/latent-diffusion" class="btn btn-primary btn-outline"><i class="icon-github"></i> Code</a>
                                </div>
                                <div style="padding-left: 20px;">
                                    <p>TL;DR: Introduced Latent Diffusion Models that operate in a lower-dimensional latent space rather than the pixel space. This innovation reduced computational costs and enabled diffusion models to handle higher-resolution images and complex tasks like text-to-image generation.</p>
                                </div>
                            </div>
                            <div class="row">
                                <div class="col-md-12">
                                    <div class="article animate-box">
                                        <h2 id="table-of-contents">
                                            Table of Contents
                                        </h2>
                                        <h4 style="padding-left: 20px;">
                                            <a href="#introduction" style="text-decoration: none; color: inherit;">1. Introduction</a>
                                        </h4>
                                    </div>
                                </div>
                            </div>
                            <div class="row">
                                <div class="col-md-12">
                                    <div class="article animate-box">
                                        <h2 id="introduction">
                                            <a href="#table-of-contents" style="text-decoration: none; color: inherit;">1. Introduction</a>
                                        </h2>
                                    </div>
                                </div>
                            </div>
                            <div class="row">
                                <div class="col-md-12">
                                    <div class="article animate-box">
                                        <h2 id="introduction">
                                            <a href="#table-of-contents" style="text-decoration: none; color: inherit;">2. Model</a>
                                        </h2>
                                        <h3>2.1 Neural Network (Denoiser): Autoencoder</h3>
                                        <p>
                                            Diffusion models <a>[Shohl-Dickstein et al. 2015]</a> are a class of generative models that operate in the pixel space. They model the data distribution by iteratively applying a series of noise levels sampled from a predefined distribution to the input image, governed by a stochastic time-dependent process that dictates the transition of data distributions over time (<a>Fig. 1</a>).
                                            However, pixel space is a high-dimensional space which makes them computationally expensive, specially during the reverse diffusion process or <i>denoising</i>.
                                            This paper uses an Autoencoder, specially, a VQ-VAE to convert the pixel space into a latent space and therefore, reduce the computational complexity of the denoising process. Also, the latent space is powerful for conditional sampling.
                                            <br><br>
                                            An autoencoder is composed by an Encoder \( \mathcal{E} \) and a Decoder \( \mathcal{D} \). The encoder maps the input image \( x \) to a lower-dimensional latent space \( z \). The decoder maps the latent space \( z \) to the output image \( x \).
                                            This paper uses a VQ-VAE in the middle of the diffusion process, which is basically using the VQ-VAE to denoise or, in other words, to predict the noise level of the input image at each time step of the reverse diffusion process.
                                            <br>
                                            In particular, the Autoencoder is a UNet-based VQ-VAE that predicts the noise level of the input image \( x_t \) that we want to remove. The input of the Unet is a noisy image which contains the noise-level correspondint to step time \( t \).
                                            This way, we input an image in the pixel space and generate a latent space representation of it.
                                            
                                            One of the core contributions of this paper is to use the latent space of the VQ-VAE for conditional sampling.
                                        </p>
                                        <h3>2.2 Training</h3>
                                        <p>
                                            Then, we can summarize the training process in 2 steps: training the autoencoder and training the diffusion process in the latent space. These steps are trained separately.
                                        </p>
                                        <h4>2.2.1 Autoencoder Training</h4>
                                        <p>
                                            A vanilla autoencoder is trained with an input \( \mathbf{x} \) and it predicts the same image \( \mathbf{\hat{x}} \).
                                            In this paper, to train the VQ-VAE along with the diffusion process which depends on time \( t \), the authors train the model in an adversarial manner.
                                            A discrimiator decoder \( \mathcal{D}_\phi \) is used to discriminate the input images \( \mathbf{x} \) from the reconstructions \( \mathbf{\hat{x}} = \mathcal{D}(\mathcal{E}(\mathbf{x}))\).
                                            <br>
                                            The autoencoder is trained with the following loss function:
                                            \[
                                                \mathcal{L}_{\text{Autoencoder}} = \min_{\mathcal{E}, D} \max_{\psi} \left( \mathcal{L}_{\text{rec}}(x, D(\mathcal{E}(x))) - \mathcal{L}_{\text{adv}}(D(\mathcal{E}(x))) + \log D_{\psi}(x) + \mathcal{L}_{\text{reg}}(x; \mathcal{E}, D) \right),
                                                \tag{Eq. 1} \label{eq:1}
                                            \]
                                            where:
                                            <ul>
                                                <li> \( \mathcal{L}_{\text{rec}}(\mathbf{x}, \mathcal{D}(\mathcal{E}(\mathbf{x}))) = \text{MSE}(\mathbf{x}, \mathcal{D}(\mathcal{E}(\mathbf{x})))\) is the reconstruction loss, which is the mean squared error between the input image and the reconstruction. It mathches the input and predicted image by the autoencoder.</li>
                                                <li> \( \mathcal{L}_{\text{reg}}(\mathbf{x};\mathcal{E},\mathcal{D}) = \text{KL}(\mathcal{E}(\mathbf{x}, \mathcal{N}(0,1))) \) is the regularization loss, which is the commitment loss of the VQ-VAE. In short, it minimizes the difference between the latents and a normal distribution. This loss, in addition to the reconstruction loss form the vanilla VAE loss.</li>
                                                <li>
                                                    \( \mathcal{L}_{\text{adv}}(\mathcal{D}(\mathcal{E}(\mathbf{x}))) = - \log D_{\psi}(\mathcal{D}(\mathcal{E}(\mathbf{x})) \approx - \log D_{\psi}(\mathbf{x}) \) is the adversarial loss, which is the log probability of the discriminator decoder \( \mathcal{D}_\phi \) which classifies the input image as real or fake. So, the total adversarial loss has the form:
                                                    \[
                                                        \min_{\mathcal{E}, D} \max_{\psi} \left( -\log [\mathcal{D}_\psi(\mathcal{D}(\mathcal{E}(\mathbf{x}))] + \log D_{\psi}(\mathcal{D}(\mathbf{x}))) \right).
                                                    \]
                                                    This is done because we want the autoencoder to reconstruct images as good as possible plus generate real images (and not fake ones).
                                                </li>
                                            </ul>
                                            Once we formulated the losses for the autoencoder, we will dive into the diffusion process.
                                            A diffusion process takes an image \( \mathbf{x} \) and applies noise to it in the forward process and denoise in the backward process (<a>Fig. 1</a>). This means that <strong>the diffusion process happens in the latent space</strong>.
                                            <div style="text-align: center; margin: 0; padding: 0; margin-top: 10px; margin-bottom: 10px;">
                                                <img src="high-resolution-image-synthesis-with-atent-diffusion-models/diffusion-process.png" 
                                                        alt="Diffusion Process" 
                                                        style="width: 40%; height: auto; display: block; margin: 0 auto; padding: 0;">
                                                <p style="font-size: small; margin: 0; padding: 0;">Fig. 1. Standard diffusion process.</p>
                                            </div>
                                            In our case, we will use an input image \( \mathbf{x} \) and pass it through the encoder to get the latents \( \mathcal{E}(\mathbf{x}) = z \). Then, we wil apply the noise to this latent (<a>Fig. 2</a>).
                                            <div style="text-align: center; margin: 0; padding: 0; margin-top: 10px; margin-bottom: 10px;">
                                                <img src="high-resolution-image-synthesis-with-atent-diffusion-models/diffusion-process-latent.png" 
                                                        alt="Latent Diffusion Process" 
                                                        style="width: 40%; height: auto; display: block; margin: 0 auto; padding: 0;">
                                                <p style="font-size: small; margin: 0; padding: 0;">Fig. 2. Diffusion process in the latent space.</p>
                                            </div>
                                        </p>
                                        <h4>2.2.2 Diffusion Training</h4>
                                        <p>
                                            During the diffusion training process, the encoder and decoder are frozen.
                                        </p>
                                        <h3>2.3 Conditioning Sampling</h3>
                                        <p>
                                            Now that we trained our model, 
                                        </p>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

    


        <div class="row">
            <div class="col-md-12 text-center">
                <p>
                    &copy;
Copyright &copy;<script>document.write(new Date().getFullYear());</script> All rights reserved | Carlos Hern&aacute;ndez Oliv&aacute;n | <a href="https://colorlib.com" target="_blank">Colorlib</a>
                </p>
            </div>
        </div>
    </div>
</div>
</footer>

</div>

<!-- jQuery -->
<script src="../js/jquery.min.js"></script>
<!-- jQuery Easing -->
<script src="../js/jquery.easing.1.3.js"></script>
<!-- Bootstrap -->
<script src="../js/bootstrap.min.js"></script>
<!-- Waypoints -->
<script src="../js/jquery.waypoints.min.js"></script>
<!-- Owl Carousel -->
<script src="../js/owl.carousel.min.js"></script>
<!-- Magnific Popup -->
<script src="../js/jquery.magnific-popup.min.js"></script>
<script src="../js/magnific-popup-options.js"></script>

<!-- Main JS (Do not remove) -->
<script src="../js/main.js"></script>

</body>
</html>